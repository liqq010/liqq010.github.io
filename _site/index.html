<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Quanquan Li</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="About" />
<meta property="og:locale" content="en_US" />
<meta property="og:site_name" content="LI Quanquan" />
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate"  title="LI Quanquan" /></head>

<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Quanquan Li</h1>
  </header>

  <header>
    <p>Email: liquanquan AT link.cuhk.edu.hk</p>
    [<a href="https://scholar.google.com/citations?user=TghYpncAAAAJ&hl" >Google Scholar</a>]
    <p></p>
  </header>

  <header>
    <h1>About me</h1>
  </header>

  <div class="post-content">
    <p>I am a senior researcher in SenseTime, which is a leading China company focused on developing AI technologies. Before that I got my B.Eng. degree from The Chinese University of Hong Kong in 2015. My research interests are deep learning and computer vision, especially object detection and model acceleration.</p>
  </div>

  <header>
    <h1>Publications</h1>
  </header>

<div>
    <ul>
        <li class="publication">
            <div class="publication-title">
                1st Place Solution of LVIS Challenge 2020: A Good Box is not a Guarantee of a Good Mask
            </div>
            <div class="publication-author">
              Jingru Tan, Gang Zhang, Hanming Deng, Changbao Wang, Lewei Lu, <strong>Quanquan Li</strong>, Jifeng Dai 
            </div>
            <div class="publication-conf">
                <i>arXiv preprint arXiv:2009.01559</i>
            </div>
            <div class="publication-link">
              [<a href="https://arxiv.org/abs/2009.01559" >tech report</a>]
            </div>
          </li>
          <p></p>    
      <li class="publication">
        <div class="publication-title">
            Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks
        </div>
        <div class="publication-author">
          Kun Yuan, <strong>Quanquan Li</strong>, Dapeng Chen, Aojun Zhou, Junjie Yan 
        </div>
        <div class="publication-conf">
            <i>arXiv preprint arXiv:2010.01097</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/2010.01097" >paper</a>]
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Learning connectivity of neural networks from a topological perspective
        </div>
        <div class="publication-author">
          Kun Yuan, <strong>Quanquan Li</strong>, Jing Shao, Junjie Yan 
        </div>
        <div class="publication-conf">
            <i>Proceedings of the European Conference on Computer Vision (ECCV) 2020</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/2008.08261" >paper</a>]        
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection
        </div>
        <div class="publication-author">
          Xin Lu, <strong>Quanquan Li</strong>, Buyu Li, Junjie Yan 
        </div>
        <div class="publication-conf">
            <i>Proceedings of the European Conference on Computer Vision (ECCV) 2020</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/2009.11528" >paper</a>]        
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            DMCP: Differentiable Markov Channel Pruning for Neural Networks
        </div>
        <div class="publication-author">
          Shaopeng Guo, Yujie Wang, <strong>Quanquan Li</strong>, Junjie Yan 
        </div>
        <div class="publication-conf">
        <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020 Oral</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/2005.03354" >paper</a>]
          [<a href="https://github.com/zx55/dmcp" >code</a>]        
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Equalization Loss for Long-Tailed Object Recognition
        </div>
        <div class="publication-author">
          Jingru Tan, Changbao Wang, Buyu Li, <strong>Quanquan Li</strong>, Wanli Ouyang, Junjie Yan 
        </div>
        <div class="publication-conf">
        <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/2003.05176" >paper</a>]
          [<a href="https://github.com/tztztztztz/eql.detectron2" >code</a>]        
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Residual Knowledge Distillation
        </div>
        <div class="publication-author">
          Mengya Gao, Yujun Shen, <strong>Quanquan Li</strong>, Chen Change Loy 
        </div>
        <div class="publication-conf">
        <i>arXiv preprint arXiv:2002.09168</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/2002.09168" >paper</a>]
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Feature Matters: A Stage-by-Stage Approach for Knowledge Transfer
        </div>
        <div class="publication-author">
          Mengya Gao, Yujun Shen, <strong>Quanquan Li</strong>, Chen Change Loy, Xiaoou Tang 
        </div>
        <div class="publication-conf">
        <i>arXiv preprint arXiv:1812.01819</i>
        </div>
        <div class="publication-link">
          [<a href="arXiv preprint arXiv:1812.01819" >paper</a>]
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Grid R-CNN Plus: Faster and Better
        </div>
        <div class="publication-author">
          Xin Lu, Buyu Li, Yuxin Yue, <strong>Quanquan Li</strong>, Junjie Yan
        </div>
        <div class="publication-conf">
        <i>arXiv preprint arXiv:1906.05688</i>
        </div>
        <div class="publication-link">
          [<a href="arXiv preprint arXiv:1906.05688" >paper</a>]
          [<a href="https://github.com/STVIR/Grid-R-CNN" >code</a>]
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Grid R-CNN
        </div>
        <div class="publication-author">
          Xin Lu, Buyu Li, Yuxin Yue, <strong>Quanquan Li</strong>, Junjie Yan
        </div>
        <div class="publication-conf">
        <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2019</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/1811.12030" >paper</a>]
          [<a href="https://github.com/STVIR/Grid-R-CNN" >code</a>]
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Mimicking very efficient network for object detection
        </div>
        <div class="publication-author">
          <strong>Quanquan Li</strong>, Shengying Jin, Junjie Yan
        </div>
        <div class="publication-conf">
        <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</i>
        </div>
        <div class="publication-link">
          [<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf" >paper</a>]
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            Jointly Learning Deep Features, Deformable Parts, Occlusion and Classification for Pedestrian Detection
        </div>
        <div class="publication-author">
          Wanli Ouyang, Hui Zhou, Hongsheng Li, <strong>Quanquan Li</strong>, Junjie Yan, Xiaogang Wang
        </div>
        <div class="publication-conf">
        <i>IEEE transactions on pattern analysis and machine intelligence 40.8 (2017): 1874-1887</i>
        </div>
        <div class="publication-link">
          [<a href="https://wlouyang.github.io/Papers/Ouyang2017JoingCNNPed.pdf">paper</a>]
          [<a href="www.ee.cuhk.edu.hk/âˆ¼wlouyang/projects/ouyangWiccv13Joint/index.html">code</a>]
        </div>
      </li>
      <p></p>
      <li class="publication">
        <div class="publication-title">
            POI: Multiple Object Tracking with High Performance Detection and Appearance Feature
        </div>
        <div class="publication-author">
          Fengwei Yu, Wenbo Li, <strong>Quanquan Li</strong>, Yu Liu, Xiaohua Shi, Junjie Yan
        </div>
        <div class="publication-conf">
        <i>European Conference on Computer Vision. Workshops (ECCV2016 workshops) 2016</i>
        </div>
        <div class="publication-link">
          [<a href="https://arxiv.org/abs/1610.06136">paper</a>]
        </div>
      </li>
    </ul>
</div>

<header>
    <h1>Competitions</h1>
  </header>

  <div>
    <ul>
    <li>Winner of <a href="https://www.lvisdataset.org/challenge_2020">LVIS Challenge 2020</a> [<a href="https://arxiv.org/abs/2009.01559">tech report</a>][<a href="https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/slides/2.1_Slides_lvisTraveler_LVIS2020.pdf">slides</a>]</li>
    <li>Winner and most innovative award of <a href="https://www.lvisdataset.org/challenge_2019">LVIS Challenge 2019</a> [<a href="https://arxiv.org/abs/1911.04692">tech report</a>]</li>
    <li>Winner of <a href="https://ai.chuangxin.com/ai_challenger?lang=en-US">AI Challenge 2017 Pose Estimation Track</a></li>
    <li>Ranked 2nd place of <a href="https://cocodataset.org/#keypoints-leaderboard">COCO Challenge 2017 Pose Estimation Track</a></li>
    <li>Winner of <a href="https://motchallenge.net/">Winner of MOT Challenge 2016</a></li>
    </ul>
  </div>


</article>
</div>

</body>

</html>
